{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from input_proc import *\n",
    "from p1_funs import persist_rets_bin, load_rets_from_bin, floatr_to_uint_minret, uint_ret_offs_tofloatr\n",
    "\n",
    "fname = Path(os.getcwd())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# answer to the question: how to store stock returns in a file\n",
    "# without losing precision best option in terms of size is to use compressed numpy array\n",
    "# column names and indices can be stored separately, e.g. in another arrays inside compressed container\n",
    "persist_rets_bin(str(fname) + r'\\floatGzipped', xret)  # 155 MB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# other ways described, some optimal for particular tasks, e.g. columnar storage for PySpark jobs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Convert pandas DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(xret)\n",
    "\n",
    "# Write PyArrow Table to Parquet file\n",
    "pq.write_table(table, 'xret.parquet')  # already less than pkl only 20% more than np compressed\n",
    "pq.write_table(table, 'xretgzipped.parquet', compression='gzip')\n",
    "# gzip only 3% improvement over uncompressed parquet, zstd same, no improvement with brotli either\n",
    "\n",
    "\n",
    "# quick search of compressors suggested to use blosc2, but suboptimal as requires C-arrays\n",
    "import blosc2 as bl2\n",
    "\n",
    "compressed = bl2.pack_array(np.ascontiguousarray(xret))  # incl. clvl9 and shuffle\n",
    "\n",
    "# Save the compressed data to a file, > 200 MB\n",
    "with open('cr_compressed_blosc.b2frame', 'wb') as f:\n",
    "    f.write(compressed)\n",
    "\n",
    "blarr = bl2.compress2(np.ascontiguousarray(arr))  # fails with Fortran array\n",
    "# savedsz = bl2.save_array(blarr, str(Path(fname, 'cr_compressed_blosc.bl2')), mode='w')\n",
    "# this works but floods console, anyway worse than npz 162 MB, so disappointing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# h5py is also a good option, but not as good as compressed numpy array,\n",
    "# leaving here for reference, listing all tried options\n",
    "import h5py\n",
    "with h5py.File(str(Path(fname, 'crDF_compressed9.h5')), \"w\") as f:\n",
    "    # same size\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xret, compression=\"gzip\", compression_opts=9)\n",
    "    dset_coefs = f.create_dataset(\"a\", data=xret, compression=\"gzip\", compression_opts=9, shuffle=True)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xret, compression=\"SZIP\", compression_opts=9)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xret, compression=\"ZFP\", compression_opts=9)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xret, compression=\"LZF\", compression_opts=9)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xret, compression=\"LZMA\", compression_opts=9)\n",
    "# enabling shuffling can potentially improve the compression ratio without modifying the original data\n",
    "#  after decompression will be the same as the original data. 177 MB, rather slow\n",
    "#  xret or xret.values, no difference\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instead of trying more compression tools incl. lossy, we can do our own simplistic one\n",
    "# reducing precision to float32 would have been too simple:\n",
    "# but decrease size of nobs * ncomps * 8 bytes for float64 from 232 to 160 MB\n",
    "persist_rets_bin(Path(fname, 'f32' + 'gzipped'), xdf.astype(np.float32))  # 80MB, half the f64 size as expected\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# also, we can use min and max values from data to create custom dtype knowing range of our values:\n",
    "np.nanmax(arr)\n",
    "np.nanmin(arr)  # but this is more like data-mining\n",
    "\n",
    "# more 'theory' solid is to make use of the fact that stock returns are > -1 (price can't go below 0)\n",
    "\n",
    "\n",
    "# numpy.uint16  # 0 to 65535, whereas uint8 is 0 to 255 (not enough for either company ids or dates)\n",
    "np.iinfo(np.uint16).max\n",
    "\n",
    "# 32-bit floating-point values cover range from 1.175494351 * 10^-38 to 3.40282347 * 10^+38,\n",
    "# which is too much for stock returns, so we may want to limit negative bound to -1 and\n",
    "# (questionably) sacrifice precision to 4 or 5 digits after the decimal point (in practice,\n",
    "# it depends on upper bound, theoretically unlimited for stock returns, but with some assumptions\n",
    "# even np.uint64 with 19 digits will have smaller output size than float64, see at the bottom).\n",
    "\n",
    "# to sum up, method: use integers to count number of steps (e.g. 0.0001 step size for\n",
    "#  4 digits after the decimal point) from minimal value, which for stock returns is -1.\n",
    "\n",
    "# so for above we can use uint16 given our data, but best to use uint32 to be on the safe side:\n",
    "np.nanmin(arr[arr>0])  # > 10**(-10) step size but < even 10**(-9)...\n",
    "np.nanmax(arr[arr<0])  # similarly, > -10**(-9) but < -10**(-10)\n",
    "np.nanmin(np.abs(arr))  # equivalent test for both bounds, can be also inferred from f64 type\n",
    "np.nanmax(np.abs(arr))  # 0.09999999999999998, so 5 digits after the decimal point is enough\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# first, use defined function to convert float64 to uint16 per this logic, to show how it works:\n",
    "\n",
    "floatr_to_uint_minret(xret.values, prec=5, minval=-0.1)  # can use for our random data\n",
    "floatr_to_uint_minret(xret.values, prec=4, minval=-1)  # appropriate defaults for stock returns (more general case)\n",
    "\n",
    "# now, show saving of data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'persist_rets_bin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mpersist_rets_bin\u001B[49m(\u001B[38;5;28mstr\u001B[39m(fname) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124muint16offst\u001B[39m\u001B[38;5;124m'\u001B[39m, xdf, to_int\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39muint16, prec\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, minval\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# 34 MB is\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m#  80% reduction in size, but obviously we lose precision, may be outside of the task scope...\u001B[39;00m\n\u001B[0;32m      3\u001B[0m persist_rets_bin(Path(fname, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mui32_p9\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgzipped\u001B[39m\u001B[38;5;124m'\u001B[39m), xdf, to_int \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39muint32, prec\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m9\u001B[39m)  \u001B[38;5;66;03m# 80MB\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'persist_rets_bin' is not defined"
     ]
    }
   ],
   "source": [
    "persist_rets_bin(str(fname) + r'\\uint16offst', xdf, to_int=np.uint16, prec=4, minval=-1)  # 34 MB is\n",
    "#  80% reduction in size, but obviously we lose precision, may be outside of the task scope...\n",
    "persist_rets_bin(Path(fname, 'ui32_p9' + 'gzipped'), xdf, to_int = np.uint32, prec=9)  # 80MB\n",
    "persist_rets_bin(Path(fname, 'ui64_p19' + 'gzipped'), xdf, to_int = np.uint64, prec=19)  # 146MB\n",
    "\n",
    "# now showcase restoring values and checking lost precision is as expected (also, covered in unit test):"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-21T15:33:31.809737600Z",
     "start_time": "2023-05-21T15:33:31.327624800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xdf2 = load_rets_from_bin('uint16offst_npcomprsd.npz', val_dtype=np.uint16, prec=4, minval=-1)  # also converts uint16 to float64\n",
    "\n",
    "prec = 4\n",
    "stepsz = 10**(-prec)  # fails with smaller rounding, 3 tests below with increasing \"difficulty\"\n",
    "\n",
    "errAssertPrec = \"reconstructed values differ from original\"\n",
    "assert np.nanmax(np.abs(xdf.values.round(4) - xdf2.values)) < stepsz, errAssertPrec\n",
    "assert np.nanmax(np.abs(xdf.values - xdf2.values)) < stepsz, errAssertPrec\n",
    "assert np.allclose(np.nan_to_num(xdf.values), np.nan_to_num(xdf2.values), atol=1e-4), errAssertPrec\n",
    "\n",
    "#  may need improvement before production use\n",
    "np.nanmean(np.abs(xdf.values - xdf2.values))  # average error\n",
    "np.nanmax( np.abs(xdf.values - xdf2.values)) # worst error"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function now also works with specified range of values\n",
    "\n",
    "\n",
    "# also, test range functionality, mapping e.g. floats betweer values [-1, 1] to ints [0, 65535] assumption of fixed step\n",
    "xret3rng = floatr_to_uint_minret(xret.values.copy(), minval=-0.07, maxval=0.07)\n",
    "xret3_orng = uint_ret_offs_tofloatr(xret3rng, minval=-0.07, maxval=0.07)\n",
    "np.nanmean(np.abs(xret.values.round(4) - xret3_orng))  # strangely on range precision is worse than on minret only, same with nanmax\n",
    "\n",
    "\n",
    "\n",
    "xret3rngu = floatr_to_uint_minret(xret.values.copy(), prec=4, minval=-1)\n",
    "xret3_orngu = uint_ret_offs_tofloatr(xret3rngu, prec=4, minval=-1)\n",
    "\n",
    "np.nanmean(np.abs(xret.values.round(4) - xret3_orngu))  # indeed, yields smallest error, same with nanmax\n",
    "\n",
    "# chk worst case max abs error\n",
    "np.nanmax(np.abs(xret.values.round(4) - xret3_orngu))  # 0.0001\n",
    "np.nanmax(np.abs(xret.values.round(4) - xret3_orng))  # 0.0001"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
