{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:06:23.993336600Z",
     "start_time": "2023-05-21T16:06:17.010689Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from input_proc import *\n",
    "from p1_funs import persist_rets_bin, load_rets_from_bin, floatr_to_uint_minret, uint_ret_offs_tofloatr\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:14:40.885222700Z",
     "start_time": "2023-05-21T16:14:34.937819800Z"
    }
   },
   "outputs": [],
   "source": [
    "# answer to the question: how to store stock returns in a file\n",
    "# without losing precision best option in terms of size is to use compressed numpy array\n",
    "# column names and indices can be stored separately, e.g. in another arrays inside compressed container\n",
    "persist_rets_bin(str(fname) + r'\\floatGzipped', xdf)  # 155 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:15:38.283542200Z",
     "start_time": "2023-05-21T16:15:29.042847300Z"
    }
   },
   "outputs": [],
   "source": [
    "# other ways described, some optimal for particular tasks, e.g. columnar storage for PySpark jobs\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Convert pandas DataFrame to PyArrow Table\n",
    "table = pa.Table.from_pandas(xdf)\n",
    "\n",
    "# Write PyArrow Table to Parquet file\n",
    "pq.write_table(table, 'xdf.parquet')  # already less than pkl and only 20% more than np compressed\n",
    "pq.write_table(table, 'xdfgzipped.parquet', compression='gzip')  # 181MB\n",
    "# gzip only 3% improvement over uncompressed parquet, zstd same, no improvement with brotli either\n",
    "\n",
    "\n",
    "# quick search of compressors suggested to use blosc2, but suboptimal as requires C-arrays\n",
    "import blosc2 as bl2\n",
    "\n",
    "compressed = bl2.pack_array(np.ascontiguousarray(xarr))  # incl. clvl9 and shuffle\n",
    "\n",
    "# Save the compressed data to a file, > 200 MB\n",
    "with open('cr_compressed_blosc.b2frame', 'wb') as f:\n",
    "    f.write(compressed)\n",
    "\n",
    "blarr = bl2.compress2(np.ascontiguousarray(xarr))  # fails with Fortran array\n",
    "# savedsz = bl2.save_array(blarr, str(Path(fname, 'cr_compressed_blosc.bl2')), mode='w')\n",
    "# this works but floods console, anyway worse than npz 162 MB, so disappointing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:18:39.756011800Z",
     "start_time": "2023-05-21T16:17:46.093744700Z"
    }
   },
   "outputs": [],
   "source": [
    "# h5py is also a good option, but not as good as compressed numpy array,\n",
    "# leaving here for reference, listing all tried options\n",
    "import h5py\n",
    "with h5py.File(str(Path(fname, 'crDF_compressed9.h5')), \"w\") as f:\n",
    "    # same size\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xdf, compression=\"gzip\", compression_opts=9)\n",
    "    dset_coefs = f.create_dataset(\"a\", data=xdf, compression=\"gzip\", compression_opts=9, shuffle=True)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xdf, compression=\"SZIP\", compression_opts=9)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xdf, compression=\"ZFP\", compression_opts=9)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xdf, compression=\"LZF\", compression_opts=9)\n",
    "    # dset_coefs = f.create_dataset(\"a\", data=xdf, compression=\"LZMA\", compression_opts=9)\n",
    "# enabling shuffling can potentially improve the compression ratio without modifying the original data\n",
    "#  after decompression will be the same as the original data. 177 MB, rather slow\n",
    "#  xdf or xarr (xdf.values) can be passed, no difference in size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:20:40.314017800Z",
     "start_time": "2023-05-21T16:20:35.089785500Z"
    }
   },
   "outputs": [],
   "source": [
    "# instead of trying more compression tools incl. lossy, we can do our own simplistic one\n",
    "# reducing precision to float32 would have been too simple:\n",
    "# but decrease size of nobs * ncomps * 8 bytes for float64 from 232 to 160 MB\n",
    "persist_rets_bin(Path(fname, 'f32' + 'gzipped'), xdf.astype(np.float32))  # 80MB, half the f64 size as expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:22:49.080022500Z",
     "start_time": "2023-05-21T16:22:48.526893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06422126285369435\n",
      "-0.061545515358494424\n",
      "65535\n",
      "4.4723612007234463e-10\n",
      "-1.0699493636284339e-10\n",
      "1.0699493636284339e-10\n",
      "0.06422126285369435\n"
     ]
    }
   ],
   "source": [
    "# also, we can use min and max values from data to create custom dtype knowing range of our values:\n",
    "print(np.nanmax(xarr))\n",
    "print(np.nanmin(xarr))  # but this is more like data-mining\n",
    "\n",
    "# more 'theory' solid is to make use of the fact that stock returns are > -1 (price can't go below 0)\n",
    "\n",
    "\n",
    "# numpy.uint16  # 0 to 65535, whereas uint8 is 0 to 255 (not enough for either company ids or dates)\n",
    "print(np.iinfo(np.uint16).max)\n",
    "\n",
    "# 32-bit floating-point values cover range from 1.175494351 * 10^-38 to 3.40282347 * 10^+38,\n",
    "# which is too much for stock returns, so we may want to limit negative bound to -1 and\n",
    "# (questionably) sacrifice precision to 4 or 5 digits after the decimal point (in practice,\n",
    "# it depends on upper bound, theoretically unlimited for stock returns, but with some assumptions\n",
    "# even np.uint64 with 19 digits will have smaller output size than float64, see at the bottom).\n",
    "\n",
    "# to sum up, method: use integers to count number of steps (e.g. 0.0001 step size for\n",
    "#  4 digits after the decimal point) from minimal value, which for stock returns is -1.\n",
    "\n",
    "# so for above we can use uint16 given our data, but best to use uint32 to be on the safe side:\n",
    "print(np.nanmin(xarr[xarr>0]))  # > 10**(-10) step size but < even 10**(-9)...\n",
    "print(np.nanmax(xarr[xarr<0]))  # similarly, > -10**(-9) but < -10**(-10)\n",
    "print(np.nanmin(np.abs(xarr)))  # equivalent test for both bounds, can be also inferred from f64 type\n",
    "print(np.nanmax(np.abs(xarr)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:26:15.995455800Z",
     "start_time": "2023-05-21T16:26:15.750397300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9835     0  8630  9695 12496  9223     0 10832     0     0]\n",
      " [    0     0     0 10308 10715 10759  9467  9490 10470  7911]\n",
      " [    0 10820 10981  9785 10001  8189 10053 10958 10381     0]\n",
      " [ 7928     0  9300     0     0 10023 11393 11178     0  9565]\n",
      " [ 7575     0  9468     0     0 10476  9989  8515     0  9779]\n",
      " [    0     0 10159     0 10390  8105  8592  7607     0 10139]\n",
      " [10048 10721  9195 11318 10724  9453  9570 11060 10165  9899]\n",
      " [ 8820 12257     0     0     0 11535  7631     0  9576     0]\n",
      " [    0  7967 11336  8428  8364 11246 10098     0 10414     0]\n",
      " [    0     0 11370 10738 11927     0     0 10207 10651  8474]]\n",
      "------------------------------  converted as will be saved with more appropriate defaults for stock returns (more general case)\n",
      "[[ 9984     0  9864  9970 10251  9923     0 10084     0     0]\n",
      " [    0     0     0 10032 10072 10077  9948  9950 10048  9792]\n",
      " [    0 10083 10099  9979 10001  9820 10006 10097 10039     0]\n",
      " [ 9794     0  9931     0     0 10003 10140 10119     0  9957]\n",
      " [ 9758     0  9948     0     0 10049 10000  9852     0  9979]\n",
      " [    0     0 10017     0 10040  9811  9860  9762     0 10015]\n",
      " [10006 10073  9920 10133 10073  9946  9958 10107 10017  9991]\n",
      " [ 9883 10227     0     0     0 10154  9764     0  9958     0]\n",
      " [    0  9798 10134  9844  9837 10126 10011     0 10042     0]\n",
      " [    0     0 10138 10075 10194     0     0 10022 10066  9848]]\n"
     ]
    }
   ],
   "source": [
    "# first, use defined function to convert float64 to uint16 per this logic, to show how it works:\n",
    "\n",
    "print(floatr_to_uint_minret(xarr, prec=5, minval=-0.1)[:10,:10])  # can use for our random data\n",
    "print('---'*10, ' converted as will be saved with more appropriate defaults for stock returns (more general case)')\n",
    "print(floatr_to_uint_minret(xarr, prec=4, minval=-1)[:10,:10])\n",
    "\n",
    "\n",
    "# now, show saving of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:26:58.141881700Z",
     "start_time": "2023-05-21T16:26:40.103624200Z"
    }
   },
   "outputs": [],
   "source": [
    "persist_rets_bin(str(fname) + r'\\uint16offst', xdf, to_int=np.uint16, prec=4, minval=-1)  # 34 MB is\n",
    "#  80% reduction in size, but obviously we lose precision, may be outside of the task scope...\n",
    "persist_rets_bin(Path(fname, 'ui32_p9' + 'gzipped'), xdf, to_int = np.uint32, prec=9)  # 78MB, see below to check precision vs. f32\n",
    "persist_rets_bin(Path(fname, 'ui64_p19' + 'gzipped'), xdf, to_int = np.uint64, prec=19)  # 146MB\n",
    "\n",
    "# now showcase restoring values and checking lost precision is as expected (also, covered in unit test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:48:59.630718400Z",
     "start_time": "2023-05-21T16:48:56.478975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5002338192945866e-05 average error\n",
      "4.9999995246557266e-05 worst error\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "xdf2 = load_rets_from_bin('uint16offst_npcomprsd.npz', val_dtype=np.uint16, prec=4, minval=-1)  # also converts uint16 to float64\n",
    "\n",
    "prec = 4\n",
    "stepsz = 10**(-prec)  # fails with smaller rounding, 3 tests below with increasing \"difficulty\"\n",
    "\n",
    "errAssertPrec = \"reconstructed values differ from original\"\n",
    "assert np.nanmax(np.abs(xdf.values.round(4) - xdf2.values)) < stepsz, errAssertPrec\n",
    "assert np.nanmax(np.abs(xdf.values - xdf2.values)) < stepsz, errAssertPrec\n",
    "assert np.allclose(np.nan_to_num(xdf.values), np.nan_to_num(xdf2.values), atol=1e-4), errAssertPrec\n",
    "\n",
    "#  may need improvement before production use\n",
    "print(np.nanmean(np.abs(xdf.values - xdf2.values)),  'average error')\n",
    "print(np.nanmax( np.abs(xdf.values - xdf2.values)), 'worst error')\n",
    "\n",
    "# but both within tolerance level:\n",
    "print((np.nanmax( np.abs(xdf.values - xdf2.values)) > np.nanmean( np.abs(xdf.values - xdf2.values)))\n",
    "        and (np.nanmax( np.abs(xdf.values - xdf2.values)) < stepsz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:49:16.058807700Z",
     "start_time": "2023-05-21T16:49:13.760265400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8626446982028533e-09 Max Abs Err for f32\n",
      "5.000001385147002e-10 Max Abs Err for ui32\n",
      "2.0565834790162216e-10 MAE for f32\n",
      "2.500288090765228e-10 MAE for ui32\n"
     ]
    }
   ],
   "source": [
    "# ui32 9-digit precision vs. f32 (only 2MB larger size for f32, but 4x worse precision):\n",
    "xarr2_f32 = np.load(file=Path(fname, 'f32' + 'gzipped' + '_npcomprsd.npz'))['arr']\n",
    "xdf2_ui32 = load_rets_from_bin(Path(fname, 'ui32_p9' + 'gzipped' + '_npcomprsd.npz'),\n",
    "                               val_dtype=np.uint32, prec=9, minval=-1)\n",
    "\n",
    "# beats on max abs error, but not on mean abs error; depends on user or use case which is more important\n",
    "print(np.nanmax(np.abs(xdf.values - xarr2_f32)), 'Max Abs Err for f32')\n",
    "print(np.nanmax(np.abs(xdf.values - xdf2_ui32.values)), 'Max Abs Err for ui32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average error better for f32, but not by much (20%) vs 4x for max abs error (where ui32 is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:51:02.208758Z",
     "start_time": "2023-05-21T16:51:01.467587900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0565834790162216e-10 MAE for f32\n",
      "2.500288090765228e-10 MAE for ui32\n"
     ]
    }
   ],
   "source": [
    "print(np.nanmean(np.abs(xdf.values - xarr2_f32)), 'MAE for f32')\n",
    "print(np.nanmean(np.abs(xdf.values - xdf2_ui32.values)), 'MAE for ui32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function now also works with not just min value and precision specified, but also a range of values min and max, (auto-inferring precision), which may be useful in some cases\n",
    "\n",
    "\n",
    "also, test range functionality, mapping e.g. floats between values [-1, 1] to ints [0, 65535] assumption of fixed step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-21T16:54:40.390895700Z",
     "start_time": "2023-05-21T16:54:37.331174100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.500605056077757e-05\n",
      "5.1057466353355424e-05\n",
      "4.1627594848535255e-17\n",
      "1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "xret3rng = floatr_to_uint_minret(xarr.copy(), minval=-0.07, maxval=0.07)\n",
    "xret3_orng = uint_ret_offs_tofloatr(xret3rng, minval=-0.07, maxval=0.07)\n",
    "print(np.nanmean(np.abs(xarr.round(4) - xret3_orng)))  # strangely on range precision is worse than on minret only, same with nanmax\n",
    "print(np.nanmax(np.abs(xarr.round(4) - xret3_orng)))  # 0.0001\n",
    "\n",
    "\n",
    "xret3rngu = floatr_to_uint_minret(xarr.copy(), prec=4, minval=-1)\n",
    "xret3_orngu = uint_ret_offs_tofloatr(xret3rngu, prec=4, minval=-1)\n",
    "\n",
    "print(np.nanmean(np.abs(xarr.round(4) - xret3_orngu)))  # indeed, yields smallest error, same with nanmax\n",
    "print(np.nanmax(np.abs(xarr.round(4) - xret3_orngu)))  # 0.0001, TBD, debugging/fix outside of task scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
